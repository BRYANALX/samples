{"cells":[{"cell_type":"markdown","source":["#ETL con Apache Koalas usando Amazon Dataset \n\nUn **ETL pipeline** es un proceso común para almacenar y produce datos más confiables y enriquecidos y que consiste en tres pasos: **Extracción** de los datos, de una o múltiples fuentes; **Transformación** de los mismos; y **Carga (o Load)** de esos datos en un destino común (ie: Data Lake). \n\nEn la siguiente demo, veremos cómo aplicar un ETL Pipeline básico en Databricks, usando Apache Koalas, y tomando como dataset un listado de Usuarios en Amazon.\n\n\n#### Explorando datos\nEl Amazon Dataset, está almacenado en el Databricks File System (o DBFS) como uno de los datasets de ejemplo que ofrece Databricks. En el path `/databricks-datasets/amazon/users` hay un listado de usuarios (con sus IDs) en formato CSV."],"metadata":{}},{"cell_type":"code","source":["print(dbutils.fs.ls(\"/databricks-datasets/amazon/users\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[FileInfo(path=&#39;dbfs:/databricks-datasets/amazon/users/_SUCCESS&#39;, name=&#39;_SUCCESS&#39;, size=0), FileInfo(path=&#39;dbfs:/databricks-datasets/amazon/users/part-r-00000-f8d9888b-ba9e-47bb-9501-a877f2574b3c.csv&#39;, name=&#39;part-r-00000-f8d9888b-ba9e-47bb-9501-a877f2574b3c.csv&#39;, size=251736212)]\n</div>"]}}],"execution_count":2},{"cell_type":"markdown","source":["## Paso 1: Extracción del Dataset\nComenzamos con la primer fase del ETL: la **Extracción** de los datos de los usuarios de Amazon, que almacenamos en memoria en un DataFrame llamado `users_df`."],"metadata":{}},{"cell_type":"code","source":["import numpy as np\nimport databricks.koalas as ks\n\nusers_path = \"/databricks-datasets/amazon/users/\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Mon Sep  7 23:52:33 2020 py4j imported\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["users_df = ks.read_csv(users_path)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["users_df.head()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>84838498</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>424687383</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>474450171</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1691900178</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>959087395</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["## Paso 2: Análisis y Transformación"],"metadata":{}},{"cell_type":"markdown","source":["La segunda fase del ETL es la **Transformación** del DataFrame de usuarios en el `unique_users_df` DataFrame, que no contiene duplicados.\n\nPrimero, verificamos el total de usuarios en `users_df`, y cuantos de ellos están repetidos o son null."],"metadata":{}},{"cell_type":"code","source":["null_values = users_df.isnull().sum()['user']\nunique_users = users_df.nunique()['user']\ntotal_users = users_df.count()['user']"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["print(\"Null values: {0}\".format(null_values))\nprint(\"Unique Users: {0}\".format(unique_users))\nprint(\"Total Users: {0}\".format(total_users))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Null values: 0\nUnique Users: 9104511\nTotal Users: 24013668\n</div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["Como el `total_users` es distinto al `unique_users`, eso significa que hay usuarios duplicados. \nCrearemos un nuevo DataFrame llamado `unique_users_df` para eliminar usuarios duplicados, y para eso usamos `unique()`."],"metadata":{}},{"cell_type":"code","source":["unique_users_df = ks.DataFrame(users_df['user'].unique())\nprint(\"Total unique users: {0}\".format(unique_users_df.count()['user']))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Total unique users: 9104511\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["unique_users_df.head(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>785573682</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1325428211</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>931960654</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>58723512</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>131984955</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1229690137</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>893812680</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>890537209</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>491959511</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1081196643</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["## Paso 3: Carga de los datos procesados\n\nEl tercer paso es la **Carga** de los datos generados, que almacenamos de forma permanente. En este caso, se guardarán en el path `/unique_users.parquet` en formato Parquet."],"metadata":{}},{"cell_type":"code","source":["amazon_path = \"/FileStore/tables/amazon\"\nunique_users_path = \"{0}/unique_users.parquet\".format(amazon_path)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":15},{"cell_type":"code","source":["unique_users_df.to_parquet(unique_users_path)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["## Paso 4: Validación de los datos almacenados (Opcional)\n\nLuego de generar los archivos finales de salida, podemos verificar que se hayan escrito correctamente."],"metadata":{}},{"cell_type":"code","source":["amazon_path = \"/FileStore/tables/amazon\"\nusers_path = \"{0}/unique_users.parquet\".format(amazon_path)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":18},{"cell_type":"code","source":["unique_users_new_df = ks.read_parquet(users_path)\nprint(\"Total unique users: \", unique_users_new_df.count())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Total unique users:  user    9104511\ndtype: int64\n</div>"]}}],"execution_count":19},{"cell_type":"markdown","source":["Como no tenemos duplicados, el nuevo dataset nos quedó con la cantidad exacta de usuarios."],"metadata":{}},{"cell_type":"markdown","source":["## Paso 5: Conclusión\nFinalmente, ¡el ETL está terminado! Ahora,e ste set de usuarios puede ser consumido o utilizado por otras notebooks o programas con distintos fines (como visualizar datos, construir modelos de Machine Learning, etc)."],"metadata":{}}],"metadata":{"name":"Data Pipeline Demo1","notebookId":4283938883895223},"nbformat":4,"nbformat_minor":0}
